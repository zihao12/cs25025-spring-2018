{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import collections\n",
    "import pickle\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters used in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', default='./')\n",
    "parser.add_argument('--save_dir', default='./')\n",
    "# Dimension of hidden layer variables h and c\n",
    "parser.add_argument('--num_units', default=128*2)\n",
    "parser.add_argument('--batch_size', default=64)\n",
    "# Number of steps in each batch for training\n",
    "parser.add_argument('--num_steps', default=75)\n",
    "parser.add_argument('--num_epochs', default=3)\n",
    "# Time step\n",
    "parser.add_argument('--lr', default=0.002)\n",
    "# Number of possible inputs/outputs \n",
    "parser.add_argument('--num_chars')\n",
    "parser.add_argument('--num_batches',default=20)\n",
    "args, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timer(start, end):\n",
    "    hrs, rem = divmod(end-start, 3600)\n",
    "    mins, secs = divmod(rem, 60)\n",
    "    print('{:0>2} hours {:0>2} minutes {:05.2f} seconds'.format(int(hrs), int(mins), secs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for handling the text data, creating vocabulary (in this case just the 65 characters). Creating batches and loading batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "\n",
    "    def __init__(self, data_dir, batch_size=64, seq_length=50, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.encoding = encoding\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.input_file = os.path.join(data_dir, 'tinyshakespeare.txt')\n",
    "        self.vocab_file = os.path.join(data_dir, 'vocab.pkl')\n",
    "        # Numeric file of characters translated to indices.\n",
    "        self.tensor_file = os.path.join(data_dir, 'data.npy')\n",
    "        \n",
    "        if not (os.path.exists(self.vocab_file) and os.path.exists(self.tensor_file)):\n",
    "            print('it seems we havent processed the text data yet: reading the shakespear')\n",
    "            self.preprocess(self.input_file, self.vocab_file, self.tensor_file)\n",
    "        else:\n",
    "            print('there are preprocessed data - lets load it')\n",
    "            self.load_preprocessed(self.vocab_file, self.tensor_file)\n",
    "\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    # Create numeric file.\n",
    "    def preprocess(self, input_file=None, vocab_file=None, tensor_file=None, saveit=True):\n",
    "        if input_file is not None:\n",
    "            self.input_file = input_file\n",
    "        if vocab_file is not None:\n",
    "            self.vocab_file = vocab_file\n",
    "        if tensor_file is not None:\n",
    "            self.tensor_file = tensor_file\n",
    "\n",
    "        with open(self.input_file, 'r') as f:\n",
    "            data = f.read()\n",
    "        #data = data.lower()\n",
    "        self.total_length = len(data)\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab_to_idx = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.idx_to_vocab = dict(zip(self.vocab_to_idx.values(), self.vocab_to_idx.keys()))\n",
    "\n",
    "        if saveit:\n",
    "            with open(self.vocab_file, 'wb') as f:  # saving dictionary so we don't compute it again\n",
    "                pickle.dump(self.chars, f)\n",
    "            self.tensor = np.array(list(map(self.vocab_to_idx.get, data)))\n",
    "            np.save(self.tensor_file, self.tensor)  # saving the numerified data\n",
    "    # Load numeric file create dictionaries for char2idx and back\n",
    "    def load_preprocessed(self, vocab_file=None, tensor_file=None):\n",
    "        if vocab_file is not None:\n",
    "            self.vocab_file = vocab_file\n",
    "        if tensor_file is not None:\n",
    "            self.tensor_file = tensor_file\n",
    "\n",
    "        with open(self.vocab_file, 'rb') as f:\n",
    "            self.chars = pickle.load(f)\n",
    "\n",
    "        # attributes\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(self.vocab_size)))\n",
    "        self.vocab_to_idx = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.idx_to_vocab = dict(zip(self.vocab_to_idx.values(), self.vocab_to_idx.keys()))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "            \n",
    "    # tensor size = the length of the entire data sequence\n",
    "    # divide into batch_size sub sequences and stack\n",
    "    # cut those by seq_length to produce batches of [batch size, seq_length] sized examples\n",
    "    def create_batches(self):\n",
    "\n",
    "        \n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        if self.num_batches == 0:\n",
    "            assert False, 'Not enough data. Make seq_length and/or batch_size smaller'\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]  # so we get an even divide\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "\n",
    "        # ydata is one step ahead of x and last item is first item of x \n",
    "        # to get sequences of same length    \n",
    "        ydata[:-1] = xdata[1:] \n",
    "        ydata[-1] = xdata[0]\n",
    "\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        \n",
    "        self.train_num_batches=np.int32(self.num_batches*.8)\n",
    "        self.test_num_batches=self.num_batches-self.train_num_batches\n",
    "        self.train_x_batches=self.x_batches[0:self.train_num_batches]\n",
    "        self.train_y_batches=self.y_batches[0:self.train_num_batches]\n",
    "        self.test_x_batches=self.x_batches[self.train_num_batches:]\n",
    "        self.test_y_batches=self.y_batches[self.train_num_batches:]\n",
    "\n",
    "        # xdata: L length\n",
    "        # xdata reshaped: batch_size, (L/batch_size) length following natural indexing\n",
    "        # np.split: into num batches batches along the width(sentence)\n",
    "\n",
    "    def next_batch_train(self):\n",
    "        x, y = self.train_x_batches[self.pointer], self.train_y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x,y\n",
    "    \n",
    "    def next_batch_test(self):\n",
    "        x, y = self.test_x_batches[self.pointer], self.test_y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x,y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(args.data_dir, batch_size=args.batch_size, seq_length=args.num_steps)\n",
    "args.num_chars = loader.vocab_size\n",
    "print('num chars',args.num_chars)\n",
    "print('num batches',loader.num_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBasicRNNCell(tf.contrib.rnn.BasicRNNCell):\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        \n",
    "        self._kernel = self.add_variable(name=\"kernel_hidden\", shape=[input_depth + self._num_units, self._num_units])\n",
    "        self._bias = self.add_variable(name=\"bias_hidden\", shape=[self._num_units], initializer=tf.zeros_initializer())\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "        \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n",
    "        \n",
    "        output = tf.tanh(tf.matmul(tf.concat([inputs, state], 1), self._kernel) + self._bias)\n",
    "\n",
    "        return output, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n",
    "\n",
    "class MyBasicLSTMCell(tf.contrib.rnn.BasicLSTMCell):\n",
    "\n",
    "    def build(self, inputs_shape):\n",
    "\n",
    "        input_depth = inputs_shape[1].value\n",
    "        self._kernel = self.add_variable(name=\"kernel\", shape=[input_depth + self._num_units, 4 * self._num_units])\n",
    "        self._bias = self.add_variable(name=\"bias\", shape=[4 * self._num_units], initializer=tf.zeros_initializer())\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, state):\n",
    "\n",
    "        one = tf.constant(1, dtype=tf.int32)\n",
    "        c, h = state\n",
    "\n",
    "        gate_inputs = tf.matmul(tf.concat([inputs, h], axis=1), self._kernel) + self._bias\n",
    "\n",
    "        input_gate_weights, input_weights, forget_gate_weights, output_gate_weights = tf.split(\n",
    "            value=gate_inputs, num_or_size_splits=4, axis=one)\n",
    "\n",
    "        # forget gating\n",
    "        forget_bias_tensor = tf.constant(1.0, dtype=forget_gate_weights.dtype)\n",
    "        forget_gate = tf.sigmoid(forget_gate_weights + forget_bias_tensor)\n",
    "        gated_memory = c * forget_gate\n",
    "\n",
    "        # input gating\n",
    "        processed_new_input = tf.tanh(input_weights)\n",
    "        input_gate = tf.sigmoid(input_gate_weights)\n",
    "        gated_input = input_gate * processed_new_input\n",
    "\n",
    "        # updating memory\n",
    "        new_c = gated_memory + gated_input\n",
    "\n",
    "        # output gating\n",
    "        processed_memory = tf.tanh(new_c)\n",
    "        output_gate = tf.sigmoid(output_gate_weights)\n",
    "        new_h = processed_memory * output_gate\n",
    "\n",
    "        new_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n",
    "\n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function incorporating chosen Cell type (LSTM, RNN). Unrolling it and adding the loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(myLSTMCell,inputs,targets):\n",
    "\n",
    "    with tf.variable_scope('embedding_matrix'):\n",
    "        embedding = tf.get_variable('embedding', [args.num_chars, args.num_units])\n",
    "        embedded_inputs = tf.nn.embedding_lookup(embedding, inputs)\n",
    "        inputs_list = tf.unstack(embedded_inputs, axis=1)  # shape: a list of [batch_size, num_units] length num_steps\n",
    "\n",
    "    with tf.variable_scope('LSTMCell') as myscope:\n",
    "        cell = myLSTMCell(args.num_units)\n",
    "        init_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "        state = init_state\n",
    "        outputs = []\n",
    "\n",
    "        for time_, input in enumerate(inputs_list):\n",
    "            if time_ > 0:\n",
    "                myscope.reuse_variables()\n",
    "           \n",
    "            output, state = cell(input, state)\n",
    "            outputs.append(output)\n",
    "    # All hidden outputs for each batch and every step in the batch are reshaped\n",
    "    # as one long matrix to be transformed to logits and compared to targets.\n",
    "        output_reshaped = tf.reshape(tf.concat(outputs, 1), [-1, args.num_units])\n",
    "\n",
    "        final_state = state\n",
    "\n",
    "    with tf.variable_scope('regression'):\n",
    "        W = tf.get_variable('W', [args.num_units, args.num_chars])\n",
    "        b = tf.get_variable('b', [args.num_chars], initializer=tf.constant_initializer(0.0))\n",
    "        logits = tf.matmul(output_reshaped, W) + b\n",
    "        prob = tf.nn.softmax(logits)\n",
    "\n",
    "    with tf.variable_scope('cost'):\n",
    "        targets_straightened = tf.reshape(targets, [-1])\n",
    "        crossentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                    labels=targets_straightened)\n",
    "        loss = tf.reduce_mean(crossentropy)\n",
    "        cost = loss/args.batch_size/args.num_steps\n",
    "\n",
    "    with tf.variable_scope('optimizer'):\n",
    "        train_step = tf.train.AdamOptimizer(args.lr).minimize(loss)\n",
    "\n",
    "    with tf.variable_scope('saver'):\n",
    "        saver = tf.train.Saver()\n",
    "    return init_state, train_step, loss, final_state, saver, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(myCell,num_batches=None):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    tf.reset_default_graph()\n",
    "    # Define the placeholders\n",
    "    with tf.variable_scope('placeholders'):\n",
    "            inputs = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "            targets = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "    # Create the network\n",
    "    init_state, train_step, loss, final_state, saver, prob=network(myCell,inputs,targets)\n",
    "    print('train_num_batches',loader.train_num_batches)\n",
    "    \n",
    "    if (num_batches is None):\n",
    "        num_batches=loader.train_num_batches\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "         \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        # computation graph for training\n",
    "        training_losses = []\n",
    "\n",
    "        for epoch in range(args.num_epochs):\n",
    "            loader.reset_batch_pointer()\n",
    "            state_ = sess.run(init_state)\n",
    "            training_loss = 0\n",
    "\n",
    "            for batch in range(num_batches):\n",
    "\n",
    "                x, y = loader.next_batch_train()\n",
    "\n",
    "                feed_dict = dict()\n",
    "                feed_dict[inputs] = x\n",
    "                feed_dict[targets] = y\n",
    "                \n",
    "                if ('RNN' in myCell.__name__):\n",
    "                    feed_dict[init_state] = state_\n",
    "                else:\n",
    "                    feed_dict[init_state.c] = state_.c\n",
    "                    feed_dict[init_state.h] = state_.h\n",
    "\n",
    "                train_loss_, state_, _ = sess.run([loss, final_state, train_step], feed_dict=feed_dict)\n",
    "                training_loss += train_loss_\n",
    "            training_loss=training_loss/num_batches\n",
    "            print('epoch:', epoch, 'loss:',  training_loss)\n",
    "            training_losses.append(training_loss)\n",
    "        saver.save(sess, os.path.join(args.save_dir, 'saved_model'))\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    timer(start_time, end_time)\n",
    "    return(training_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "re_lstm_basic = trainer(MyBasicLSTMCell,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_rnn_basic = trainer(MyBasicRNNCell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload model and run a simulated prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Synthesize(MyCell,init_string=\"None\"):\n",
    "    tf.reset_default_graph()\n",
    "    num_steps_bak=args.num_steps\n",
    "    batch_size_bak=args.batch_size\n",
    "    args.num_steps=1\n",
    "    args.batch_size=1\n",
    "    with tf.variable_scope('placeholders'):\n",
    "            inputs = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "            targets = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "    init_state, train_step, loss, final_state, saver, prob=network(MyCell,inputs,targets)\n",
    "\n",
    "    # Define initialization\n",
    "    if (init_string is None):\n",
    "        initialization = 'Where are you going today?'\n",
    "    else:\n",
    "        initialization = init_string\n",
    "    loader= TextLoader(args.data_dir, batch_size=1, seq_length=1)\n",
    "\n",
    "    forecast_data=np.array(list(map(loader.vocab_to_idx.get, initialization)))\n",
    "    print(forecast_data)\n",
    "    forecast_range = 100\n",
    "    top_k=5\n",
    " \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Load saved model\n",
    "        saver.restore(sess, 'saved_model')\n",
    "        state_ = sess.run(init_state)\n",
    "\n",
    "        # Run rnn on initialization data to get final hidden state before simulation\n",
    "        state_ = sess.run(init_state)\n",
    "        for i in range(forecast_data.shape[0]):\n",
    "\n",
    "            feed_dict = dict()\n",
    "            # Feed current predicted\n",
    "            feed_dict[inputs] = forecast_data[i].reshape(args.batch_size, args.num_steps)\n",
    "            if ('RNN' in MyCell.__name__):\n",
    "                feed_dict[init_state] = state_\n",
    "            else:\n",
    "                feed_dict[init_state.c] = state_.c\n",
    "                feed_dict[init_state.h] = state_.h\n",
    "            # Get new hidden state and prediction probabilities\n",
    "            predicted_prob, state_ = sess.run([prob, final_state], feed_dict=feed_dict)\n",
    "\n",
    "        # last state of this step becomes first state of simulation\n",
    "\n",
    "        for i in range(forecast_range):\n",
    "\n",
    "            feed_dict = dict()\n",
    "            # Feed current predicted\n",
    "            feed_dict[inputs] = forecast_data[-args.num_steps:].reshape(args.batch_size, args.num_steps)\n",
    "            if ('RNN' in MyCell.__name__):\n",
    "                feed_dict[init_state] = state_\n",
    "            else:\n",
    "                feed_dict[init_state.c] = state_.c\n",
    "                feed_dict[init_state.h] = state_.h\n",
    "            predicted_prob, state_ = sess.run([prob, final_state], feed_dict=feed_dict)\n",
    "\n",
    "            predicted_prob = predicted_prob.ravel()\n",
    "            # Simulate from top top_k probs\n",
    "            predicted_prob[np.argsort(predicted_prob)[:-top_k]] = 0\n",
    "            predicted_prob = predicted_prob/np.sum(predicted_prob)\n",
    "            sample = np.random.choice(args.num_chars, 1, p=predicted_prob)[0]\n",
    "\n",
    "\n",
    "            forecast_data = np.hstack((forecast_data, sample))\n",
    "\n",
    "    forecasted_chars = np.asarray([loader.idx_to_vocab[elem] for elem in forecast_data])\n",
    "\n",
    "    print(''.join(forecasted_chars))\n",
    "  \n",
    "    args.num_steps=num_steps_bak\n",
    "    args.batch_size=batch_size_bak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get loss on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tester(MyCell, num_batches=None):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.variable_scope('placeholders'):\n",
    "            inputs = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "            targets = tf.placeholder(tf.int32, [args.batch_size, args.num_steps])\n",
    "    init_state, train_step, loss, final_state, saver, prob=network(MyCell,inputs,targets)\n",
    "\n",
    "    # Define initialization\n",
    "    initialization = 'Where are you going today?'\n",
    "    loader= TextLoader(args.data_dir, batch_size=args.batch_size, seq_length=args.num_steps)\n",
    "\n",
    "    forecast_data=np.array(list(map(loader.vocab_to_idx.get, initialization)))\n",
    "    print(forecast_data)\n",
    "    forecast_range = 100\n",
    "    top_k=5\n",
    " \n",
    "    if (num_batches is None):\n",
    "        num_batches=loader.test_num_batches\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Load saved model\n",
    "        saver.restore(sess, 'saved_model')\n",
    "        state_ = sess.run(init_state)\n",
    "\n",
    "        loader.reset_batch_pointer()\n",
    "        \n",
    "         # Get test error loss\n",
    "        test_loss = 0\n",
    "        print('num_batches',num_batches)\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "\n",
    "            x, y = loader.next_batch_test()\n",
    "\n",
    "            feed_dict = dict()\n",
    "            feed_dict[inputs] = x\n",
    "            feed_dict[targets] = y\n",
    "\n",
    "            if ('RNN' in MyCell.__name__):\n",
    "                feed_dict[init_state] = state_\n",
    "            else:\n",
    "                feed_dict[init_state.c] = state_.c\n",
    "                feed_dict[init_state.h] = state_.h\n",
    "\n",
    "            test_loss_, state_= sess.run([loss, final_state], feed_dict=feed_dict)\n",
    "            test_loss += test_loss_\n",
    "        test_loss=test_loss/num_batches\n",
    "        print('test loss:',  test_loss)\n",
    "\n",
    "\n",
    "    \n",
    "    return(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tester(MyBasicLSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Synthesize(MyBasicLSTMCell,\"What is the meaning of life\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
